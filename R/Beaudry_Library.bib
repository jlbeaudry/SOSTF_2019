
@misc{noauthor_zotero_nodate,
  title = {Zotero | {{Your}} Personal Research Assistant},
  howpublished = {https://www.zotero.org/start},
  file = {/Users/jbeaudry/Zotero/storage/RIH9B8B2/start.html}
}

@article{saraiva_development_nodate,
  title = {Development and Validation of the {{Eyewitness Metamemory Scale}}},
  volume = {0},
  copyright = {\textcopyright{} 2019 The Authors Applied Cognitive Psychology Published by John Wiley \& Sons Ltd},
  issn = {1099-0720},
  abstract = {Metamemory can be defined as the knowledge about one's memory capabilities and about strategies that can aid memory. In this paper, we describe the development and validation of the Eyewitness Metamemory Scale (EMS), tailored specifically for use in face memory and eyewitness identification settings. Participants (N = 800) completed the EMS and other measures on general metamemory. Results from exploratory and confirmatory factor analysis revealed good factorial validity, internal consistency, and content validity. The EMS items emerged into three distinct factors: memory contentment, memory discontentment, and memory strategies. The EMS is a brief and easily administrable questionnaire that might be used to assess self-ratings of face recognition capacity and use of strategies to encode faces.},
  language = {en},
  number = {0},
  journal = {Applied Cognitive Psychology},
  doi = {10.1002/acp.3588},
  author = {Saraiva, Renan Benigno and van Boeijen, Inger Mathilde and Hope, Lorraine and Horselenberg, Robert and Sauerland, Melanie and van Koppen, Peter J.},
  keywords = {eyewitness identification,face recognition,factor analysis,metamemory,scale development},
  file = {/Users/jbeaudry/Zotero/storage/IK38N23V/Saraiva et al. - Development and validation of the Eyewitness Metam.pdf;/Users/jbeaudry/Zotero/storage/EIZD7PMN/acp.html}
}

@article{bates_effects_1999,
  title = {The Effects of Participation and Presentation Media on the Eyewitness Memory of Children},
  volume = {51},
  copyright = {1999 Australian Psychological Society},
  issn = {1742-9536},
  abstract = {The effects of participation and presentation media on eyewitness memory were examined in children aged 5 to 6 and 11 to 12 years. The children witnessed a staged event in one of three conditions: live-bystander, live-participant, or a video of the live-participant event. Older children recalled more correct information and provided a higher proportion of correct recall than did younger children. Age differences also emerged for the yes/no recognition responses, and were influenced by the presentation condition. The older children had a higher recognition rate than the younger children in the live-participant and video conditions, but no differences emerged in the live-bystander condition. Participation effects were found across both age groups, as both younger and older children in the live-participant condition recalled more correct information than did those in the live-bystander and video conditions. Further research is warranted to investigate the ecological validity of laboratory findings based on witnesses who are tested using live-bystander or videotaped events.},
  language = {en},
  number = {2},
  journal = {Australian Journal of Psychology},
  doi = {10.1080/00049539908255338},
  author = {Bates, Jennifer L. and Ricciardelli, Lina A. and Clarke, Valerie A.},
  year = {1999},
  keywords = {eyetracking; children},
  pages = {71-76},
  file = {/Users/jbeaudry/Zotero/storage/662LSP7Q/00049539908255338.html}
}

@misc{noauthor_anderson-louis1994_article_thegraduatestudentexperiencean.pdf_nodate,
  title = {Anderson-{{Louis1994}}\_{{Article}}\_{{TheGraduateStudentExperienceAn}}.Pdf},
  abstract = {Shared with Dropbox},
  language = {en},
  journal = {Dropbox},
  howpublished = {https://www.dropbox.com/s/rp1r6rizmn6qwek/Anderson-Louis1994\_Article\_TheGraduateStudentExperienceAn.pdf?dl=0},
  file = {/Users/jbeaudry/Zotero/storage/7FH9XALQ/Anderson-Louis1994_Article_TheGraduateStudentExperienceAn.html}
}

@article{anderson_graduate_1994,
  title = {The Graduate Student Experience and Subscription to the Norms of Science},
  volume = {35},
  issn = {1573-188X},
  abstract = {This paper examines the normative orientations of doctoral students with respect to academic research. In particular, it analyzes the effects of graduate department structure, department climate, and students' mentoring experiences on students' subscription to the traditional norms of science and to alternative counternorms. Findings are based on data from a nationwide survey of students in chemistry, civil engineering, microbiology, and sociology. The analysis demonstrates substantial ambivalence among graduate students about the traditional norms of academic research. It also reveals significant differences in the normative orientations of U.S. and international students.},
  language = {en},
  number = {3},
  journal = {Research in Higher Education},
  doi = {10.1007/BF02496825},
  author = {Anderson, Melissa S. and Louis, Karen Seashore},
  month = may,
  year = {1994},
  keywords = {Academic Research,Civil Engineering,Doctoral Student,Graduate Student,International Student},
  pages = {273-299}
}

@book{noauthor_why_nodate,
  title = {Why {{I Am Not}} a {{Scientist}}},
  abstract = {This lively and provocative book casts an anthropological eye on the field of science in a wide-ranging and innovative discussion that integrates philosophy, history, sociology, and auto-ethnography. Jonathan Marks examines biological anthropology, the history of the life sciences, and the literature of science studies while upending common understandings of science and culture with a mixture of anthropology, common sense, and disarming humor.},
  language = {en},
  file = {/Users/jbeaudry/Zotero/storage/9UZLJU4Q/why-i-am-not-a-scientist.html}
}

@article{tomcho_apas_2009,
  title = {{{APA}}'s {{Learning Objectives}} for {{Research Methods}} and {{Statistics}} in {{Practice}}: {{A Multimethod Analysis}}},
  volume = {36},
  issn = {0098-6283, 1532-8023},
  shorttitle = {{{APA}}'s {{Learning Objectives}} for {{Research Methods}} and {{Statistics}} in {{Practice}}},
  language = {en},
  number = {2},
  journal = {Teaching of Psychology},
  doi = {10.1080/00986280902739693},
  author = {Tomcho, Thomas J. and Rice, Diana and Foels, Rob and Folmsbee, Leah and Vladescu, Jason and Lissman, Rachel and Matulewicz, Ryan and Bopp, Kara},
  month = apr,
  year = {2009},
  keywords = {syllabi},
  pages = {84-89},
  file = {/Users/jbeaudry/Zotero/storage/ZX3PTLTK/Tomcho et al. - 2009 - APA's Learning Objectives for Research Methods and.pdf}
}

@article{homa_analysis_2013,
  title = {An {{Analysis}} of {{Learning Objectives}} and {{Content Coverage}} in {{Introductory Psychology Syllabi}}},
  volume = {40},
  issn = {0098-6283, 1532-8023},
  abstract = {Introductory psychology is one of the most popular undergraduate courses and often serves as the gateway to choosing psychology as an academic major. However, little research has examined the typical structure of introductory psychology courses. The current study examined student learning objectives (SLOs) and course content in introductory psychology syllabi (N \textonequarter{} 158). SLOs were mapped to the APA Guidelines for the Undergraduate Psychology Major. Content analysis was based on the principles for quality undergraduate education promulgated by the American Psychological Association. Over 50\% of the syllabi contained objectives specific to the science and application of psychology (knowledge base, research methods, and application). Analysis of content coverage revealed instructors spent significantly more time on topics related to physiological and cognitive psychology and spent significantly less time on topics related to the history and scope of psychology, research methods, and developmental psychology. The current study also explored the influence of instructor specialty area on content coverage.},
  language = {en},
  number = {3},
  journal = {Teaching of Psychology},
  doi = {10.1177/0098628313487456},
  author = {Homa, Natalie and Hackathorn, Jana and Brown, Carrie M. and Garczynski, Amy and Solomon, Erin D. and Tennial, Rachel and Sanborn, Ursula A. and Gurung, Regan A. R.},
  month = jul,
  year = {2013},
  keywords = {syllabi},
  pages = {169-174},
  file = {/Users/jbeaudry/Zotero/storage/YGM92DXW/Homa et al. - 2013 - An Analysis of Learning Objectives and Content Cov.pdf}
}

@article{sullivan_examining_2003,
  title = {Examining Paradigmatic Development in Criminology and Criminal Justice: {{A}} Content Analysis of Research Methods Syllabi in Doctoral Programs},
  volume = {14},
  issn = {1051-1253, 1745-9117},
  shorttitle = {Examining Paradigmatic Development in Criminology and Criminal Justice},
  language = {en},
  number = {2},
  journal = {Journal of Criminal Justice Education},
  doi = {10.1080/10511250300085791},
  author = {Sullivan, Christopher J. and Maxfield, Michael G.},
  month = nov,
  year = {2003},
  pages = {269-285},
  file = {/Users/jbeaudry/Zotero/storage/3HN32H74/Sullivan and Maxfield - 2003 - Examining paradigmatic development in criminology .pdf}
}

@article{baguley_re-evaluating_2019,
  title = {Re-Evaluating How to Measure Jurors' Comprehension and Application of Jury Instructions},
  issn = {1068-316X, 1477-2744},
  abstract = {In order for jurors to decide a legally correct verdict, they must comprehend and apply jury instructions. To date, empirical research has focused on jurors' comprehension of instructions. However, it is difficult to know how well jurors actually comprehend instructions, because the tests currently used by researchers to measure jurors' comprehension provide different estimates of jurors' comprehension. It is also difficult to know the degree to which jurors apply instructions, because researchers have not directly examined this question. This article reviews the current tests used to measure jurors' comprehension of instructions, and the current methods used to make inferences about jurors' application of instructions. It then critically analyses these approaches, and recommends ways to improve these approaches in future research, to enable researchers to draw more precise conclusions about the quality of jurors' decision-making.},
  language = {en},
  journal = {Psychology, Crime \& Law},
  doi = {10.1080/1068316X.2019.1634195},
  author = {Baguley, Chantelle M. and McKimmie, Blake M. and Masser, Barbara M.},
  month = jun,
  year = {2019},
  keywords = {comprehension},
  pages = {1-14},
  file = {/Users/jbeaudry/Zotero/storage/F2KWN747/Baguley et al. - 2019 - Re-evaluating how to measure jurorsâ€™ comprehension.pdf}
}

@article{haller_misinterpretations_2002,
  title = {Misinterpretations of {{Significance}}: {{A Problem Students Share}} with {{Their Teachers}}?},
  volume = {7},
  abstract = {The use of significance tests in science has been debated from the invention of these tests until the present time. Apart from theoretical critiques on their appropriateness for evaluating scientific hypotheses, significance tests also receive criticism for inviting misinterpretations. We presented six common misinterpretations to psychologists who work in German universities and found out that they are still surprisingly widespread \textendash{}even among instructors who teach statistics to psychology students. Although these misinterpretations are well documented among students, until now there has been little research on pedagogical methods to remove them. Rather, they are considered ``hard facts'' that are impervious to correction. We discuss the roots of these misinterpretations and propose a pedagogical concept to teach significance tests, which involves explaining the meaning of statistical significance in an appropriate way.},
  language = {en},
  number = {1},
  author = {Haller, Heiko and Krauss, Stefan},
  year = {2002},
  pages = {20},
  file = {/Users/jbeaudry/Zotero/storage/66EYUXFN/Haller and Krauss - 2002 - Misinterpretations of Significance A Problem Stud.pdf}
}

@article{chopik_how_2018,
  title = {How (and {{Whether}}) to {{Teach Undergraduates About}} the {{Replication Crisis}} in {{Psychological Science}}},
  volume = {45},
  issn = {0098-6283, 1532-8023},
  abstract = {Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining how to communicate these issues to undergraduates and what effect this has on their attitudes toward the field. We developed and validated a 1-hr lecture communicating issues surrounding the replication crisis and current recommendations to increase reproducibility. Pre- and post-lecture surveys suggest that the lecture serves as an excellent pedagogical tool. Following the lecture, students trusted psychological studies slightly less but saw greater similarities between psychology and natural science fields. We discuss challenges for instructors taking the initiative to communicate these issues to undergraduates in an evenhanded way.},
  language = {en},
  number = {2},
  journal = {Teaching of Psychology},
  doi = {10.1177/0098628318762900},
  author = {Chopik, William J. and Bremner, Ryan H. and Defever, Andrew M. and Keller, Victor N.},
  month = apr,
  year = {2018},
  pages = {158-163},
  file = {/Users/jbeaudry/Zotero/storage/MKPD5PP2/Chopik et al. - 2018 - How (and Whether) to Teach Undergraduates About th.pdf}
}

@article{soderberg_using_2018,
  title = {Using {{OSF}} to {{Share Data}}: {{A Step}}-by-{{Step Guide}}},
  volume = {1},
  issn = {2515-2459, 2515-2467},
  shorttitle = {Using {{OSF}} to {{Share Data}}},
  abstract = {Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data.},
  language = {en},
  number = {1},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245918757689},
  author = {Soderberg, Courtney K.},
  month = mar,
  year = {2018},
  pages = {115-120},
  file = {/Users/jbeaudry/Zotero/storage/GRUE5LJ6/Soderberg - 2018 - Using OSF to Share Data A Step-by-Step Guide.pdf}
}

@article{meyer_practical_2018,
  title = {Practical {{Tips}} for {{Ethical Data Sharing}}},
  volume = {1},
  issn = {2515-2459, 2515-2467},
  abstract = {This Tutorial provides practical dos and don'ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say\textemdash{}and what not to say\textemdash{}in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing ``public'' data.},
  language = {en},
  number = {1},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245917747656},
  author = {Meyer, Michelle N.},
  month = mar,
  year = {2018},
  pages = {131-144},
  file = {/Users/jbeaudry/Zotero/storage/WWDAELD6/Meyer - 2018 - Practical Tips for Ethical Data Sharing.pdf}
}

@article{mcgrath_data_2018,
  title = {Data Sharing in Qualitative Research: Opportunities and Concerns},
  volume = {7},
  issn = {23127996},
  shorttitle = {Data Sharing in Qualitative Research},
  abstract = {Data sharing is increasingly practiced by researchers and mandated by research funders as well as scientific journals. However, data sharing within qualitative research paradigms is less common, and sharing interview data has particular challenges. Earlier debate has pointed to the value of data sharing for discouraging research fraud and permitting critical scrutiny. We elaborate on this discussion by highlighting the value of data sharing for cumulative science, for re-use, and to maximise the value of the participants' contribution. We review methods and possibilities for sharing interview data, and give concrete recommendations for mitigating risks to the participants. In conclusion, we find that sharing of interview data is possible, valuable, and ethical, and serves a purpose for both journals and researchers.},
  language = {en},
  number = {4},
  journal = {MedEdPublish},
  doi = {10.15694/mep.2018.0000255.1},
  author = {McGrath, Cormac and Nilsonne, Gustav},
  year = {2018},
  file = {/Users/jbeaudry/Zotero/storage/MBURTRBH/McGrath and Nilsonne - 2018 - Data sharing in qualitative research opportunitie.pdf}
}

@article{arslan_how_2019,
  title = {How to {{Automatically Document Data With}} the {\emph{Codebook}} {{Package}} to {{Facilitate Data Reuse}}},
  volume = {2},
  issn = {2515-2459, 2515-2467},
  abstract = {Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once\textemdash{}by their creators\textemdash{}and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.},
  language = {en},
  number = {2},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245919838783},
  author = {Arslan, Ruben C.},
  month = jun,
  year = {2019},
  pages = {169-187},
  file = {/Users/jbeaudry/Zotero/storage/7V2KJ69S/Arslan - 2019 - How to Automatically Document Data With the icod.pdf}
}

@article{nicolas_exploring_2019,
  title = {Exploring {{Research}}-{{Methods Blogs}} in {{Psychology}}: {{Who Posts What About Whom}}, and {{With What Effect}}?},
  issn = {1745-6916, 1745-6924},
  shorttitle = {Exploring {{Research}}-{{Methods Blogs}} in {{Psychology}}},
  abstract = {During the methods crisis in psychology and other sciences, much discussion developed online in forums such as blogs and other social media. Hence, this increasingly popular channel of scientific discussion itself needs to be explored to inform current controversies, record the historical moment, improve methods communication, and address equity issues. Who posts what about whom, and with what effect? Does a particular generation or gender contribute more than another? Do blogs focus narrowly on methods, or do they cover a range of issues? How do they discuss individual researchers, and how do readers respond? What are some impacts? Web-scraping and textanalysis techniques provide a snapshot characterizing 41 current research-methods blogs in psychology. Bloggers mostly represented psychology's traditional leaderships' demographic categories: primarily male, mid- to late career, associated with American institutions, White, and with established citation counts. As methods blogs, their posts mainly concern statistics, replication (particularly statistical power), and research findings. The few posts that mentioned individual researchers substantially focused on replication issues; they received more views, social-media impact, comments, and citations. Male individual researchers were mentioned much more often than female researchers. Further data can inform perspectives about these new channels of scientific communication, with the shared aim of improving scientific practices.},
  language = {en},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691619835216},
  author = {Nicolas, Gandalf and Bai, Xuechunzi and Fiske, Susan T.},
  month = jun,
  year = {2019},
  keywords = {blogs},
  pages = {174569161983521},
  file = {/Users/jbeaudry/Zotero/storage/YW8J67DK/Nicolas et al. - 2019 - Exploring Research-Methods Blogs in Psychology Wh.pdf}
}

@article{faigman_group_2013,
  title = {Group to {{Individual}} ({{G2i}}) {{Inference}} in {{Scientific Expert Testimony}}},
  issn = {1556-5068},
  language = {en},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2298909},
  author = {Faigman, David L. and Monahan, John and Slobogin, Christopher},
  year = {2013},
  file = {/Users/jbeaudry/Zotero/storage/GBS638N8/Faigman et al. - 2013 - Group to Individual (G2i) Inference in Scientific .pdf}
}

@article{sandve_ten_2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1003285},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  month = oct,
  year = {2013},
  pages = {e1003285},
  file = {/Users/jbeaudry/Zotero/storage/SMJJBSKT/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.PDF}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  volume = {2},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  number = {8},
  journal = {PLoS Medicine},
  author = {Ioannidis, John P A},
  year = {2005},
  pages = {6},
  file = {/Users/jbeaudry/Zotero/storage/3EKATBA6/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  volume = {349},
  issn = {0036-8075, 1095-9203},
  language = {en},
  number = {6251},
  journal = {Science},
  doi = {10.1126/science.aac4716},
  author = {{Open Science Collaboration}},
  month = aug,
  year = {2015},
  pages = {aac4716-aac4716},
  file = {/Users/jbeaudry/Zotero/storage/JX7NCWPZ/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{de_vries_cumulative_2018,
  title = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression},
  volume = {48},
  issn = {0033-2917, 1469-8978},
  shorttitle = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments},
  language = {en},
  number = {15},
  journal = {Psychological Medicine},
  doi = {10.1017/S0033291718001873},
  author = {{de Vries}, Y. A. and Roest, A. M. and {de Jonge}, P. and Cuijpers, P. and Munaf{\`o}, M. R. and Bastiaansen, J. A.},
  month = nov,
  year = {2018},
  pages = {2453-2455},
  file = {/Users/jbeaudry/Zotero/storage/3RSD4TZG/de Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf}
}

@article{camerer_evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  volume = {2},
  issn = {2397-3374},
  language = {en},
  number = {9},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0399-z},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  month = sep,
  year = {2018},
  pages = {637-644},
  file = {/Users/jbeaudry/Zotero/storage/9S8A47UN/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{simmons_false-positive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  volume = {22},
  issn = {0956-7976, 1467-9280},
  shorttitle = {False-{{Positive Psychology}}},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  language = {en},
  number = {11},
  journal = {Psychological Science},
  doi = {10.1177/0956797611417632},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  month = nov,
  year = {2011},
  pages = {1359-1366},
  file = {/Users/jbeaudry/Zotero/storage/9YP7WCTY/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{john_measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  volume = {23},
  issn = {0956-7976, 1467-9280},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  language = {en},
  number = {5},
  journal = {Psychological Science},
  doi = {10.1177/0956797611430953},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  month = may,
  year = {2012},
  pages = {524-532},
  file = {/Users/jbeaudry/Zotero/storage/YA4HJQ4A/John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf}
}

@article{fraser_questionable_nodate,
  title = {Questionable Research Practices in Ecology and Evolution},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  language = {en},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  pages = {16},
  file = {/Users/jbeaudry/Zotero/storage/6ZI5RFWF/Fraser et al. - Questionable research practices in ecology and evo.pdf}
}

@techreport{philpot_postprint_2019,
  type = {Preprint},
  title = {Postprint - {{Would I}} Be {{Helped}}? {{Cross}}-{{National CCTV Footage Shows That Intervention Is}} the {{Norm}} in {{Public Conflicts}}},
  shorttitle = {Postprint - {{Would I}} Be {{Helped}}?},
  abstract = {Half a century of research on bystander behavior concludes that individuals are less likely to intervene during an emergency when in the presence of others than when alone. By contrast, little is known regarding the aggregated likelihood that at least someone present at an emergency will do something to help. The importance of establishing this aggregated intervention baseline is not only of scholarly interest, but is also the most pressing question for actual public victims\textemdash{}will I receive help if needed? The current paper describes the largest systematic study of real-life bystander intervention in actual public conflicts captured by surveillance cameras. Using a unique cross-national video dataset from the United Kingdom, Netherlands, and South Africa (N = 219), we show that in nine-out-of-ten public conflicts, at least one bystander, but typically several, will do something to help. We record similar likelihoods of intervention across the three national contexts, which differ greatly in levels of recorded violent crime. Finally, we find that increased bystander presence is related to a greater likelihood that someone will intervene. Taken together these findings allay the widespread fear that bystanders rarely intervene to help. We argue that it is time for psychology to change the narrative away from an absence of help and towards a new understanding of what makes intervention successful or unsuccessful.},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/nqscj},
  author = {Philpot, Richard and Liebst, Lasse Suonper{\"a} and Levine, Mark and Bernasco, Wim and Lindegaard, Marie Rosenkrantz},
  month = mar,
  year = {2019}
}

@techreport{saraiva_eyewitness_2019,
  type = {Preprint},
  title = {Eyewitness Metamemory Predicts Identification Performance in Biased and Unbiased Lineups},
  abstract = {Distinguishing accurate from inaccurate identifications is a challenging issue in the criminal justice system, especially for biased police lineups. That is because biased lineups undermine the diagnostic value of accuracy postdictors such as confidence and decision time. Here, we aimed to test general and eyewitness-specific self-ratings of memory capacity as potential estimators of identification performance that are unaffected by lineup bias. Participants (N = 744) completed a metamemory assessment consisting of the Multifactorial Metamemory Questionnaire and the Eyewitness Metamemory Scale and took part in a standard eyewitness paradigm. Following the presentation of a mock-crime video, they viewed either biased or unbiased lineups. Self-ratings of discontentment with eyewitness memory ability were indicative of identification accuracy for both biased and unbiased lineups. Participants who scored low on eyewitness metamemory factors also displayed a stronger confidence-accuracy calibration than those who scored high. These results suggest a promising role for self-ratings of memory capacity in the evaluation of eyewitness identifications, while also advancing theory on self-assessments for different memory systems.},
  language = {en},
  institution = {{Open Science Framework}},
  doi = {10.31219/osf.io/tay75},
  author = {Saraiva, Renan Benigno and {van Boeijen}, Inger Mathilde and Hope, Lorraine and Sauerland, Melanie and Horselenberg, Robert and {van Koppen}, Peter},
  month = jul,
  year = {2019},
  file = {/Users/jbeaudry/Desktop/Preprint - Saraiva et al (2019) Eyewitness metamemory predicts identificaiton performance in biased and unbiased lineups.pdf}
}

@techreport{saraiva_using_2019,
  type = {Preprint},
  title = {Using General and Eyewitness-Specific Metamemory Assessments to Estimate Performance in Multiple Identifications},
  abstract = {Identifications made by eyewitnesses are compelling evidence for prosecuting a suspect, but inaccurate identifications can have severe consequences including the conviction of innocent persons. Based on metacognitive frameworks, we aimed to distinguish accurate from inaccurate identification decisions by using metamemory assessments. Participants (n = 203) first completed an assessment of general and eyewitness-specific metamemory domains, followed by eight successive lineup identifications. We found that self-rated ability in the eyewitness memory domain was predictive of correct identifications, false identifications, confidence and confidence-accuracy calibration. Curiously, higher self-rated ability in the more general memory domain was predictive of fewer correct identifications. We discuss the potential applied value of metamemory assessments as predictors of correct and false lineup identifications, as well as theoretical contributions to underlying mechanisms of confidence.},
  language = {en},
  institution = {{Open Science Framework}},
  doi = {10.31219/osf.io/pnhm3},
  author = {Saraiva, Renan Benigno and {van Boeijen}, Inger Mathilde and Hope, Lorraine and Horselenberg, Robert and {van Koppen}, Peter},
  month = jul,
  year = {2019},
  file = {/Users/jbeaudry/Desktop/Preprint - Saraiva et al (2019) Using General and Eyewitness-specific metamemory assessments to estimate performance in multiple identifications.pdf}
}

@article{bryan_replicator_2019,
  title = {Replicator {{Degrees}} of {{Freedom Allow Publication}} of {{Misleading}} '{{Failures}} to {{Replicate}}'},
  issn = {1556-5068},
  language = {en},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3408200},
  author = {Bryan, Christopher and Yeager, David S. and O'Brien, Joseph},
  year = {2019},
  file = {/Users/jbeaudry/Downloads/SSRN-id3408200.pdf}
}

@inproceedings{rozin_social_2001,
  title = {Social {{Psychology}} and {{Science}} : {{Some Lessons From Solomon Asch Paul Rozin}}},
  shorttitle = {Social {{Psychology}} and {{Science}}},
  abstract = {This article presents a methodological critique of the predominant research paradigms in modern social psychology, particularly social cognition, taking the approach of Solomon Asch as a more appropriate model. The critique has 2 parts. First, the dominant model of science in the field is appropriate only for a well-developed science, in which basic, real-world phenomena have been identified, important invariances in these phenomena have been documented, and appropriate model systems that capture the essence of these phenomena have been developed. These requirements are not met for most of the phenomena under study in social psychology. Second, the model of science in use is a caricature of the actual scientific process in well-developed sciences such as biology. Such research is often not model or even hypothesis driven, but rather relies on ``informed curiosity'' to motivate research. Descriptive studies are considered important and make up a substantial part of the literature, and there is less exclusive reliance on experiment. The two parts of the critique are documented by analysis of articles in appropriate psychology and biology journals. The author acknowledges that important and high quality work is currently being done in social psychology, but believes that the field has maladaptively narrowed the range of the phenomena and methodological approaches that it deems acceptable or optimal.},
  author = {Rozin, Paul},
  year = {2001},
  keywords = {Asch conformity experiments,Caricatures,Codependency (Psychology),Description,Display resolution,Document completion status - Documented,Experiment,Journal,Psychology; Social,Requirement,Science,Social cognition},
  file = {/Users/jbeaudry/Zotero/storage/LV6TXZCR/Rozin - 2001 - Social Psychology and Science  Some Lessons From .pdf}
}

@article{jupe_science_2019,
  title = {Science or Pseudoscience? {{A}} Distinction That Matters for Police Officers, Lawyers and Judges},
  issn = {10.1080/13218719.2019.1618755},
  shorttitle = {Science or Pseudoscience?},
  abstract = {(2019). Science or pseudoscience? A distinction that matters for police officers, lawyers and judges. Psychiatry, Psychology and Law. Ahead of Print.},
  language = {en},
  journal = {Psychiatry, Psychology and Law},
  author = {Jupe, Louise Marie and Denault, Vincent},
  month = aug,
  year = {2019},
  file = {/Users/jbeaudry/Zotero/storage/ZAKBVGFQ/full.html}
}

@techreport{masuzzo_you_2017,
  type = {Preprint},
  title = {Do You Speak Open Science? {{Resources}} and Tips to Learn the Language},
  shorttitle = {Do You Speak Open Science?},
  abstract = {The internet era, large-scale computing and storage resources, mobile devices, social media, and their high uptake among different groups of people, have all deeply changed the way knowledge is created, communicated, and further deployed. These advances have enabled a radical transformation of the practice of science, which is now more open, more global and collaborative, and closer to society than ever. Open science has therefore become an increasingly important topic. Moreover, as open science is actively pursued by several high-profile funders and institutions, it has fast become a crucial matter to all researchers. However, because this widespread interest in open science has emerged relatively recently, its definition and implementation are constantly shifting and evolving, sometimes leaving researchers in doubt about how to adopt open science, and which are the best practices to follow.},
  language = {en},
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.2689v1},
  author = {Masuzzo, Paola and Martens, Lennart},
  month = jan,
  year = {2017},
  file = {/Users/jbeaudry/Zotero/storage/27ED5YHF/Masuzzo and Martens - 2017 - Do you speak open science Resources and tips to l.pdf}
}

@misc{noauthor_preregistration_nodate,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}} | {{Elsevier Enhanced Reader}}},
  language = {en},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S1364661319301846?token=D790206C04102D9B2617F2A429C0F6399E04AC900711EEF41C2B674B413E670383F8D5C73E9210A64ADBA47F0B0D79ED},
  doi = {10.1016/j.tics.2019.07.009},
  file = {/Users/jbeaudry/Zotero/storage/5JY6UKCP/NosekEtAl.TiCS.pdf;/Users/jbeaudry/Zotero/storage/BQ47NZMS/S1364661319301846.html}
}

@article{brysbaert_how_2019,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Reference Tables},
  volume = {2},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  issn = {2514-4820},
  shorttitle = {How Many Participants Do We Have to Include in Properly Powered Experiments?},
  abstract = {Article: How many participants do we have to include in properly powered experiments?  A tutorial of power analysis with reference tables},
  language = {en},
  number = {1},
  journal = {Journal of Cognition},
  doi = {10.5334/joc.72},
  author = {Brysbaert, Marc},
  month = jul,
  year = {2019},
  pages = {16},
  file = {/Users/jbeaudry/Zotero/storage/8238ZMCK/Brysbaert - 2019 - How many participants do we have to include in pro.pdf;/Users/jbeaudry/Zotero/storage/8CG9CK6U/joc.html}
}

@article{weston_recommendations_2018,
  title = {Recommendations for Increasing the Transparency of Analysis of Pre-Existing Datasets},
  abstract = {Secondary data analysis, or the analysis of pre-existing data, can be a powerful tool for the resourceful researcher. Never has this been more true than now, when technological advances allow for easier sharing of data across labs and continents and the mining of large sources of ``pre-existing data''. However, secondary data analysis is often ignored as a methodological tool, either when developing new open science practices or improving analytic methods for robust data analysis. In this paper, we hope to provide researchers with the knowledge necessary to incorporate secondary data analysis into their toolbox. Specifically, we define secondary data analysis as a tool and in relation to other common forms of analysis (including exploratory and confirmatory, observational and experimental). We highlight the advantages and disadvantages of this tool. We describe how engagement in transparency can improve and alter our interpretations of results from secondary data analysis and provide resources for robust data analysis. We close by suggesting ways in which subfields and institutions could address and improve the use of secondary data analysis.},
  doi = {10.31234/osf.io/zmt3q},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  month = jul,
  year = {2018},
  keywords = {secondary data analyses},
  file = {/Users/jbeaudry/Zotero/storage/E9DIMQQ7/Weston et al. - 2018 - Recommendations for increasing the transparency of.pdf;/Users/jbeaudry/Zotero/storage/K97S96DH/zmt3q.html}
}

@misc{resnick_more_2018,
  title = {More Social Science Studies Just Failed to Replicate. {{Here}}'s Why This Is Good.},
  abstract = {What scientists learn from failed replications: how to do better science.},
  journal = {Vox},
  howpublished = {https://www.vox.com/science-and-health/2018/8/27/17761466/psychology-replication-crisis-nature-social-science},
  author = {Resnick, Brian},
  month = aug,
  year = {2018},
  file = {/Users/jbeaudry/Zotero/storage/P4AE3Q73/psychology-replication-crisis-nature-social-science.html}
}

@article{susa_matching_2019,
  title = {{{MATCHING FACES TO ID PHOTOS}}: {{THE INFLUENCE OF MOTIVATION ON CROSS}}- {{RACE IDENTIFICATION}}},
  language = {en},
  author = {Susa, Kyle J and Gause, Chase A and Dessenberger, Steven J},
  year = {2019},
  pages = {11},
  file = {/Users/jbeaudry/Zotero/storage/7XLS3MQN/Susa et al. - 2019 - MATCHING FACES TO ID PHOTOS THE INFLUENCE OF MOTI.pdf}
}

@techreport{cruwell_7_2018,
  type = {Preprint},
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  author = {Cr{\"u}well, Sophia and {van Doorn}, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  month = nov,
  year = {2018},
  keywords = {reading list}
}

@article{vallano_familiar_2019,
  title = {Familiar Eyewitness Identifications: {{The}} Current State of Affairs.},
  volume = {25},
  issn = {1939-1528, 1076-8971},
  shorttitle = {Familiar Eyewitness Identifications},
  abstract = {This article examines ``familiar identifications,'' or identifications where an eyewitness explicitly states that she has seen the perpetrator before the crime. The first section of this article reviews the literature on familiar identification accuracy. Although these identifications can be of high accuracy they are far from infallible, particularly in cases of minimal prior exposure and poor viewing conditions during the crime. Limitations and areas for future research are discussed. Specifically, we note the ill-defined nature of familiarity within the research and its oft-misleading treatment as a dichotomous construct, and therefore emphasize its continuous nature and recommend more ecologically valid research with ``ground truth'' data be conducted to examine how its differing degrees interact with other estimator variables. The second section of this article reviews how federal and state courts commonly deal with familiar identifications. The case law reveals that many courts believe any amount of familiarity enhances eyewitness memory and identification accuracy, which sometimes (correctly or incorrectly) lessens concerns over other variables known to lower identification accuracy (e.g., poor viewing conditions, a showup identification procedure). Consequentially, courts sometimes use familiarity to deny motions to suppress eyewitness evidence along with refusing to admit eyewitness experts and jury instructions. We discuss how well courts' decisions align with the research while providing concrete recommendations for expert witnesses and legal officials in familiar identification cases. Specifically, we argue that claimed familiarity should be examined for its veracity and its effects on identification accuracy be evaluated in relation to other estimator and system variables.},
  language = {en},
  number = {3},
  journal = {Psychology, Public Policy, and Law},
  doi = {10.1037/law0000204},
  author = {Vallano, Jonathan P. and Slapinski, Kristen A. and Steele, Lisa J. and Briggs, Abigail P. and Pozzulo, Joanna D.},
  month = aug,
  year = {2019},
  pages = {128-146},
  file = {/Users/jbeaudry/Zotero/storage/WBJQKKHP/Vallano et al. - 2019 - Familiar eyewitness identifications The current s.pdf}
}

@article{fidler_epistemic_2017,
  title = {The {{Epistemic Importance}} of {{Establishing}} the {{Absence}} of an {{Effect}}},
  abstract = {In psychology as in many other sciences, Popperian rhetoric remains strong, even though Popperian practice has never been. Here, we provide an introduction to the four main approaches to epistemic justification, outlining the importance of null results in each and emphasizing the importance of each approach in developing a cumulative scientific literature. We argue that whether or not we subscribe to the Popperian Hypothetico-Deductive (HD) model of science, there is value in adopting Popper's advice about creating bold conjectures and risky tests for establishing the absence (or presence) of effects. However, the most popular approach to statistical testing, Null Hypothesis Significance Testing practice fails at both, and has arguably supported the censoring of null results from our scientific literature. Allowing null results into the scientific literature is essential for a cumulative science to function. However, we argue that even a repaired Popperian HD process won't offer much advice about what are interesting and important absences (or presences) to pursue. For answers to those fundamental questions, we need to appeal to other forms of epistemic justification such as those presented in this article.},
  doi = {10.31219/osf.io/4ga56},
  author = {Fidler, Fiona and Thorn, Felix Singleton and Barnett, Ashley and Kambouris, Steven and Kruger, Ariel},
  month = nov,
  year = {2017},
  keywords = {null effects,philosophy of science},
  file = {/Users/jbeaudry/Zotero/storage/CTHCZ7CU/Fidler et al. - 2017 - The Epistemic Importance of Establishing the Absen.pdf;/Users/jbeaudry/Zotero/storage/KIDCXEW5/4ga56.html}
}

@article{fidler_epistemic_nodate,
  title = {The {{Epistemic Importance}} of {{Establishing}} the {{Absence}} of an {{Effect}}},
  language = {en},
  author = {Fidler, Fiona and Thorn, Felix Singleton and Barnett, Ashley and Kambouris, Steven and Kruger, Ariel},
  pages = {8},
  file = {/Users/jbeaudry/Zotero/storage/FHFLQ26I/Fidler et al. - The Epistemic Importance of Establishing the Absen.pdf}
}

@article{bottesini_participants_nodate,
  title = {Do Participants Care If We P-Hack Their Data?},
  language = {en},
  author = {Bottesini, Julia and Vazire, Simine},
  pages = {24}
}

@article{bottesini_participants_nodate-1,
  title = {Do {{Participants Care If We}} P-{{Hack Their Data}}? {{A Registered Report}}},
  language = {en},
  author = {Bottesini, Julia G},
  pages = {1},
  file = {/Users/jbeaudry/Zotero/storage/87J238QI/Bottesini - Do Participants Care If We p-Hack Their Data A Re.pdf;/Users/jbeaudry/Zotero/storage/KDR8THMZ/Bottesini and Vazire - Do participants care if we p-hack their data.pdf}
}

@article{lakens_too_2017,
  title = {Too {{True}} to Be {{Bad}}: {{When Sets}} of {{Studies With Significant}} and {{Nonsignificant Findings Are Probably True}}},
  volume = {8},
  issn = {1948-5506},
  shorttitle = {Too {{True}} to Be {{Bad}}},
  abstract = {Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or ``too good to be true'') that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or ``too true to be bad.'' As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.},
  language = {en},
  number = {8},
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617693058},
  author = {Lakens, Dani{\"e}l and Etz, Alexander J.},
  month = nov,
  year = {2017},
  pages = {875-881},
  file = {/Users/jbeaudry/Zotero/storage/V6GVENU3/Lakens and Etz - 2017 - Too True to be Bad When Sets of Studies With Sign.pdf}
}

@misc{smith_2_nodate,
  title = {(2) ({{PDF}}) {{Distinguishing Between Investigator Discriminability}} and {{Eyewitness Discriminability}}: {{A Method}} for {{Creating Full Receiver Operating Characteristic Curves}} of {{Lineup Identification Performance}}},
  shorttitle = {(2) ({{PDF}}) {{Distinguishing Between Investigator Discriminability}} and {{Eyewitness Discriminability}}},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/publication/335207593\_Distinguishing\_Between\_Investigator\_Discriminability\_and\_Eyewitness\_Discriminability\_A\_Method\_for\_Creating\_Full\_Receiver\_Operating\_Characteristic\_Curves\_of\_Lineup\_Identification\_Performance},
  author = {Smith, Andrew and Yang, Yueran and Wells, Gary},
  keywords = {ROC},
  file = {/Users/jbeaudry/Zotero/storage/TNVNQYZB/335207593_Distinguishing_Between_Investigator_Discriminability_and_Eyewitness_Discriminability_.html}
}

@techreport{chin_forensic_2019,
  type = {Preprint},
  title = {Forensic {{Science}} Needs {{Registered Reports}}},
  abstract = {The registered report (RR) format is rapidly being adopted by researchers and journals in several scientific fields. RRs flip the peer review process, with reviewers evaluating proposed methods, rather than the data and findings. Editors then accept or reject articles based on the pre-data collection review. Accordingly, RRs reduce the incentive for researchers to exaggerate their findings, and they make any data-driven changes to the methods and analysis more conspicuous. They also reduce publication bias, ensuring studies with null or otherwise unfavorable results are published. RRs are being used in many fields to improve research practices and increase confidence in study findings. The authors suggest RRs ought to be the default way in which validation studies are conducted and reported in forensic science. They produce more reliable findings, advance criminal justice values, and will lead to several efficiencies in the research process.},
  institution = {{LawArXiv}},
  doi = {10.31228/osf.io/yzt2q},
  author = {Chin, Jason and McFadden, Rory and Edmond, Gary},
  month = oct,
  year = {2019}
}

@article{makin_ten_2019,
  title = {Ten Common Statistical Mistakes to Watch out for When Writing or Reviewing a Manuscript},
  volume = {8},
  issn = {2050-084X},
  abstract = {Inspired by broader efforts to make the conclusions of scientific research more robust, we have compiled a list of some of the most common statistical mistakes that appear in the scientific literature. The mistakes have their origins in ineffective experimental designs, inappropriate analyses and/or flawed reasoning. We provide advice on how authors, reviewers and readers can identify and resolve these mistakes and, we hope, avoid them in the future.},
  journal = {eLife},
  doi = {10.7554/eLife.48175},
  author = {Makin, Tamar R and {Orban de Xivry}, Jean-Jacques},
  editor = {Rodgers, Peter and Parsons, Nick and Holmes, Nick},
  month = oct,
  year = {2019},
  keywords = {analysis,causality,null results,p-hacking,power,statistics},
  pages = {e48175},
  file = {/Users/jbeaudry/Zotero/storage/GB4NKDND/Makin and Orban de Xivry - 2019 - Ten common statistical mistakes to watch out for w.pdf}
}

@article{lee_new_nodate,
  title = {New Signal Detection Theory-Based Framework for Eyewitness Performance in Lineups.},
  volume = {43},
  issn = {1573-661X},
  number = {5},
  journal = {Law and Human Behavior},
  doi = {10.1037/lhb0000343},
  author = {Lee, Jungwon and Penrod, Steven D.},
  year = {20190801},
  keywords = {SDT},
  pages = {436},
  file = {/Users/jbeaudry/Zotero/storage/RQH5HB6A/2019-43943-001.pdf}
}

@article{kaplan_likelihood_2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  volume = {10},
  issn = {1932-6203},
  abstract = {Background We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. Methods We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs {$>\$$}500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality. Results 17 of 30 studies (57\%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8\%) trials published after 2000 ({$\chi$}2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings. Conclusions The number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.},
  language = {en},
  number = {8},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0132382},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  year = {05-Aug-2015},
  keywords = {null effects,clinical trials},
  pages = {e0132382},
  file = {/Users/jbeaudry/Zotero/storage/4AR8J3J7/Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.pdf;/Users/jbeaudry/Zotero/storage/DGVT4674/article.html}
}

@article{rouder_minimizing_2018,
  title = {Minimizing {{Mistakes In Psychological Science}}},
  abstract = {Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms such as the open-science movement.  Part of this challenge in today's landscape is using new technologies such as cloud storage and computer automation.  Here we discuss a few practices designed to increase the reliability of scientific labs by focusing on what technologies and elements minimize common, ordinary mistakes.  We borrow principles from the Theory of High-Reliability Organizations which has been used to characterize operational practices in high-risk environments such as aviation and healthcare.  From these principles, we focus on five elements: 1. implementing a lab culture focused on learning from mistakes; 2. using computer automation in data and meta-data collection wherever possible; 3. standardizing organization strategies; 4. using coded rather than menu-driven analyses; 5. developing expanded documents that record how analyses were performed.},
  doi = {10.31234/osf.io/gxcy5},
  author = {Rouder, Jeffrey and Haaf, Julia M. and Snyder, Hope K.},
  month = mar,
  year = {2018},
  keywords = {lab culture},
  file = {/Users/jbeaudry/Zotero/storage/XCNXC3RR/Rouder et al. - 2018 - Minimizing Mistakes In Psychological Science.pdf;/Users/jbeaudry/Zotero/storage/JEXYE3JC/gxcy5.html}
}

@article{wagge_publishing_2019,
  title = {Publishing {{Research With Undergraduate Students}} via {{Replication Work}}: {{The Collaborative Replications}} and {{Education Project}}},
  volume = {10},
  issn = {1664-1078},
  shorttitle = {Publishing {{Research With Undergraduate Students}} via {{Replication Work}}},
  abstract = {Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2019.00247},
  author = {Wagge, Jordan R. and Brandt, Mark J. and Lazarevic, Ljiljana B. and Legate, Nicole and Christopherson, Cody and Wiggins, Brady and Grahe, Jon E.},
  year = {2019},
  keywords = {Open Science,pedagogy,Projects,Psychology,Publishing,Replication,Students,Teaching,undergraduates,collaborative,crep},
  file = {/Users/jbeaudry/Zotero/storage/ZSCK88WV/Wagge et al. - 2019 - Publishing Research With Undergraduate Students vi.pdf}
}

@misc{noauthor_notitle_nodate,
  howpublished = {https://psyarxiv.com/9mpbn/},
  keywords = {meta-analyses},
  file = {/Users/jbeaudry/Zotero/storage/2HFYAYJK/9mpbn.html}
}

@article{smaldino_natural_nodate,
  title = {The Natural Selection of Bad Science},
  volume = {3},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing\textemdash{}no deliberate cheating nor loafing\textemdash{}by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  number = {9},
  journal = {Royal Society Open Science},
  doi = {10.1098/rsos.160384},
  author = {Smaldino, Paul E. and McElreath, Richard},
  keywords = {AIMOS},
  pages = {160384},
  file = {/Users/jbeaudry/Zotero/storage/PD6VKH6J/Smaldino and McElreath - The natural selection of bad science.pdf;/Users/jbeaudry/Zotero/storage/EMZH55KI/rsos.html}
}

@article{rahal_understanding_2019,
  title = {Understanding Cognitive and Affective Mechanisms in Social Psychology through Eye-Tracking},
  volume = {85},
  issn = {0022-1031},
  abstract = {Social psychological research is increasingly interested in the cognitive and affective processes underlying human behavior in social environments. To match this emerging interest, social psychology is embracing new methodological approaches. We identify eye-tracking as an unobtrusive, direct and fine-grained process tracing technique with promising implications for these new developments. In particular, eye-tracking helps researchers avoid relying on self-report measures alone and otherwise necessary interruptions of the processes they aim to observe and understand. In order to enable social psychologists to effectively use eye-tracking, we provide a systematic review of commonly used measures. Following an introduction of the basic principles and assumptions underlying the use of eye-tracking generally, we review eye-tracking measures addressing concepts of interest for many core theories of social psychology. Specifically, we introduce options to measure processing depth and decision effort, information weighting, search strategies, cognitive load and arousal. We showcase potential uses in exemplary research questions, providing a starting point for how to select appropriate measures and tailor designs to future applications of eye-tracking to social psychology. Further, we critically discuss the limitations and auxiliary assumptions on which the introduced measures are based. Finally, we illustrate the use of eye-tracking with examples from contemporary psychological research with relevance for social psychology, and conclude with an outlook for potential benefits of the use of eye-tracking methods in core topics of social psychology.},
  language = {en},
  journal = {Journal of Experimental Social Psychology},
  doi = {10.1016/j.jesp.2019.103842},
  author = {Rahal, Rima-Maria and Fiedler, Susann},
  month = nov,
  year = {2019},
  keywords = {Social cognition,Cognitive processes,Eye-tracking,Process tracing,social psyc},
  pages = {103842},
  file = {/Users/jbeaudry/Zotero/storage/WZB7CDU3/Rahal and Fiedler - 2019 - Understanding cognitive and affective mechanisms i.pdf;/Users/jbeaudry/Zotero/storage/JZRMNXCH/S0022103119300782.html}
}

@techreport{szollosi_preregistration_2019,
  type = {Preprint},
  title = {Preregistration Is Redundant, at Best},
  abstract = {The key implication argued by proponents of preregistration is that it improves the diagnosticity of statistical tests [1]. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depend entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses).},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/x36pz},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle and Shiffrin, Rich and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  month = oct,
  year = {2019},
  keywords = {navarro}
}

@techreport{jaeger_can_2019,
  type = {Preprint},
  title = {Can We Reduce Facial Biases? {{Persistent}} Effects of Facial Trustworthiness on Sentencing Decisions},
  shorttitle = {Can We Reduce Facial Biases?},
  abstract = {Trait impressions from faces influence many consequential decisions even in situations in which they have poor diagnostic value and in which decisions should not be based on a person's appearance. Here, we test (a) whether people rely on facial appearance when making legal sentencing decisions and (b) whether two types of interventions\textemdash{}educating decision-makers and changing the accessibility of facial information\textemdash{}reduces the influence of facial stereotypes. We first introduce a novel legal decision-making paradigm with which we measure reliance on facial appearance. Results of a pretest (n = 320) show that defendants with an untrustworthy (vs. trustworthy) facial appearances are found guilty more often. We then test the effectiveness of different interventions in reducing the influence of facial stereotypes. Educating participants about the biasing effects of facial stereotypes reduces the explicit belief that personality is reflected in facial features, but does not reduce the influence of facial appearance on verdicts (Study 1, n = 979). In Study 2 (n = 975), we present information sequentially to disrupt the intuitive accessibility of trait impressions. Participants indicate an initial verdict based on case-relevant information and a final verdict based on all information (including facial photographs). The wide majority of initial sentences were not revised and therefore unbiased. However, most revised sentences were in line with facial stereotypes (e.g., a guilty verdict for an untrustworthy-looking defendant). On average, this actually increased facial bias in verdicts. Together, our findings highlight the persistent influence of facial appearance on legal sentencing decisions.},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/a8w2d},
  author = {Jaeger, Bastian and Todorov, Alexander and Evans, Anthony M and {van Beest}, Ilja},
  month = nov,
  year = {2019}
}

@article{crede_questionable_2019,
  title = {Questionable Research Practices When Using Confirmatory Factor Analysis},
  volume = {34},
  issn = {0268-3946},
  abstract = {Purpose The purpose of this paper is to describe common questionable research practices (QRPs) engaged in by management researchers who use confirmatory factor analysis (CFA) as part of their analysis. Design/methodology/approach The authors describe seven questionable analytic practices and then review one year of journal articles published in three top-tier management journals to estimate the base rate of these practices. Findings The authors find that CFA analyses are characterized by a high base rate of QRPs with one practice occurring for over 90 percent of all assessed articles. Research limitations/implications The findings of this paper call into question the validity and trustworthiness of results reported in much of the management literature. Practical implications The authors provide tentative guidelines of how editors and reviewers might reduce the degree to which the management literature is characterized by these QRPs. Originality/value This is the first paper to estimate the base rate of six QRPs relating to the widely used analytic tool referred to as CFA in the management literature.},
  number = {1},
  journal = {Journal of Managerial Psychology},
  doi = {10.1108/JMP-06-2018-0272},
  author = {Crede, Marcus and Harms, Peter},
  month = jan,
  year = {2019},
  keywords = {Psychometrics,Research methods,Scale development,Structural equation modelling,CFA},
  pages = {18-30},
  file = {/Users/jbeaudry/Zotero/storage/X5SKWX8K/Crede and Harms - 2019 - Questionable research practices when using confirm.pdf}
}

@techreport{yarkoni_generalizability_2019,
  type = {Preprint},
  title = {The {{Generalizability Crisis}}},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned\textemdash{}that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that most inferential statistical tests in psychology fail to meet this basic condition. I demonstrate how foundational assumptions of the "random effects" model used pervasively in psychology impose far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints dramatically inflates false positive rates and routinely leads researchers to draw sweeping verbal generalizations that lack any meaningful connection to the statistical quantities they are putatively based on. I argue that the routine failure to consider the generalizability of one's conclusions from a statistical perspective lies at the root of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jqw35},
  author = {Yarkoni, Tal},
  month = nov,
  year = {2019}
}

@article{nyman_distance_nodate,
  title = {The Distance Threshold of Reliable Eyewitness Identification.},
  volume = {43},
  issn = {1573-661X},
  number = {6},
  journal = {Law and Human Behavior},
  doi = {10.1037/lhb0000342},
  author = {Nyman, Thomas J. and Lampinen, James Michael and Antfolk, Jan and Korkman, Julia and Santtila, Pekka},
  year = {20190711},
  pages = {527},
  file = {/Users/jbeaudry/Zotero/storage/D52VPH93/2019-38765-001.pdf}
}

@article{chorn_variations_nodate,
  title = {Variations in Reliability and Validity Do Not Influence Judge, Attorney, and Mock Juror Decisions about Psychological Expert Evidence.},
  volume = {43},
  issn = {1573-661X},
  number = {6},
  journal = {Law and Human Behavior},
  doi = {10.1037/lhb0000345},
  author = {Chorn, Jacqueline Austin and Kovera, Margaret Bull},
  year = {20190916},
  pages = {542},
  file = {/Users/jbeaudry/Zotero/storage/WE4ELN26/2019-55518-001.pdf}
}

@article{ariel_preventing_2019,
  title = {Preventing Treatment Spillover Contamination in Criminological Field Experiments: The Case of Body-Worn Police Cameras},
  volume = {15},
  issn = {1573-3750, 1572-8315},
  shorttitle = {Preventing Treatment Spillover Contamination in Criminological Field Experiments},
  abstract = {Objectives A central issue in experiments is protecting the integrity of causal identification from treatment spillover effects. The objective of this article is to demonstrate a bright line beyond which spillover of treatment renders experimental results misleading. We focus on a highly publicized recent test of police body cameras that violated the key assumption of a valid experiment: independence of treatment conditions for each unit of analysis.
Methods In this article, we set out arguments for and against particular units of random assignment in relation to protecting against spillover effects that violate the Stable Unit Treatment Value Assumption (SUTVA).
Results Comparisons to methodological solutions from other disciplines demonstrate several ways of dealing with interference in experiments, all of which give priority to causal identification over sample size as the best pathway to statistical power.
Conclusions Researchers contemplating which units of analysis to randomize can use the case of police body-worn cameras to argue against research designs that guarantee large spillover effects.},
  language = {en},
  number = {4},
  journal = {Journal of Experimental Criminology},
  doi = {10.1007/s11292-018-9344-4},
  author = {Ariel, Barak and Sutherland, Alex and Sherman, Lawrence W.},
  month = dec,
  year = {2019},
  pages = {569-591},
  file = {/Users/jbeaudry/Dropbox/Articles/Ariel, Sutherland, & Sherman, 2019 - Spillover Contamination - Body Worn Cameras - JEC.pdf}
}

@article{hamm_body-worn_2019,
  title = {Do Body-Worn Cameras Reduce Eyewitness Cooperation with the Police? {{An}} Experimental Inquiry},
  volume = {15},
  issn = {1573-3750, 1572-8315},
  shorttitle = {Do Body-Worn Cameras Reduce Eyewitness Cooperation with the Police?},
  abstract = {Objectives The current research adds to the literature addressing police body-worn cameras (BWCs) by experimentally evaluating their effect on an interaction that has, to date, received relatively little systematic, empirical attention: police\textendash{}eyewitness interactions. Although research suggests that BWCs generally have positive effects, legal scholars and media professionals have long argued that deploying cameras in this context may backfire, especially by chilling public willingness to speak with police.
Method The current study utilized an online national convenience sample (N = 508) to test the effect of four factors that were varied across seven mock interview video conditions on participants' willingness to cooperate, the amount of information provided, accuracy and confidence in an eyewitness identification task, and perceptions like procedural fairness and trust. We hypothesized that the presence and activation of the camera would have positive effects on the interaction.
Results Regarding the factors, the manipulated presence of a recording camera had the most consistent positive impact. Whether the camera was present, and the participant's awareness of the camera and the fact that it was recording were also tested, but these comparisons were less likely to reach statistical significance. Regarding the conditions, the best outcomes were associated with officers who turned on the camera and did not explain why. Conversely, the worst outcomes were associated with officers who turned off the cameras without explanation.
Conclusions Our results suggest that the positive effects of BWCs may extend to police\textendash{}eyewitness interactions and reveal no evidence of a chilling effect on eyewitness-relevant outcomes.},
  language = {en},
  number = {4},
  journal = {Journal of Experimental Criminology},
  doi = {10.1007/s11292-019-09356-3},
  author = {Hamm, J. A. and D'Annunzio, A. M. and Bornstein, B. H. and Hoetger, L. and Herian, M. N.},
  month = dec,
  year = {2019},
  pages = {685-701},
  file = {/Users/jbeaudry/Dropbox/Articles/Hamm et al, 2019 - Body Worn Cameras & Eyewitnesses - JEC.pdf}
}

@article{lilburn_cultural_2019,
  title = {Cultural {{Problems Cannot Be Solved}} with {{Technical Solutions Alone}}},
  volume = {2},
  issn = {2522-087X},
  abstract = {A crisis in psychology has provoked researchers to seek remedies for bad practices that might damage the integrity of the discipline as a whole. The ardor for wholesale reform has led to a suite of proposed technical solutions, some of which are considered in the context of computational modeling by the target article. Any technical solution, however, must be placed within a larger cultural and scientific context to be effective (or, indeed, meaningful at all). Many of the suggestions presented in the target article represent good practice in computational cognitive modeling but, even then, still require some amount of nuance in the consideration of the relationship between practice and theory. We consider two examples\textemdash{}model preregistration and bookending\textemdash{}as a means of examining the limits of any proposed technical solution.},
  language = {en},
  number = {3},
  journal = {Computational Brain \& Behavior},
  doi = {10.1007/s42113-019-00036-z},
  author = {Lilburn, Simon D. and Little, Daniel R. and Osth, Adam F. and Smith, Philip L.},
  month = dec,
  year = {2019},
  keywords = {Open science,Philosophy of science,Preregistration,Reproducibility crisis},
  pages = {170-175},
  file = {/Users/jbeaudry/Zotero/storage/J3QJTZ8E/Lilburn et al. - 2019 - Cultural Problems Cannot Be Solved with Technical .pdf}
}


